# Поиск похожих изображений

## Введение

 * Что хотим делать?

## Поиск через векторизацию

### Простые векторизации

 * Гистограмма цветов
 * Фурье, Вильетт, прочее
 * Разложение лиц на PCA.

### CNN как векторизатор

#### Дескрипторы классификационной CNN
Если вспомнить классификационные нейросети (например, которые обучались на ImageNet), у них у всех в самом конце есть полносвязный слой, который производит классификацию. Этому слою на вход приходит некоторый вещественный вектор, и на выходе мы получаем "вероятности" принадлежности классам. Так вот этот вектор можно использовать как векторизированное представление картинки. Давайте подумаем, почему это может получаться.

#### Вспоминаем задачу классификации
Когда мы говорим про классификацию, мы уменьшаем кроссэнтропию. Давайте распишем это с учётом последнего полносвязного слоя (строчки матрицы обозначчим `W_i`). Сфокусируемся на каком-нибудь одном классе (всё равно все минимизировать) и разобьём логарифм на два. Получаем разницу двух вещей, получаем, что мы хотим максимизировать вот этот логарифм, и минимизировать другой. С первым довольно просто, там логарифм сокращается с экспонентой. Получается, что мы хотим максимизировать скалярное произведение между нашим дескриптором и `W_i` строчкой матрицы линейного слоя, которая относится к выбранному классу. С другим немного сложнее, но можно сказать, что логарифм и экспонента монотонные преобразования, и на самом деле мы хотим минимизировать всевозможные скалярные произведения дескриптора и строчек матрицы `W`. 
Это говорит на самом деле о том, что мы хотим сделать такой дескриптор (и матрицу `W`), чтобы для нашего выбранного класса `i` этот дескриптор был больше похож на `i`-ю строчку `W`, чем на другие.
В итоге мы получаем, что можно брать предобученную сеть, прогонять через неё всю картинку, брать предпоследний выход (дескриптор), и использовать это как векторизированное представление. Правда проблема будет в том, что надо знать, что за задача изначально решалась сетью. Потому что для модели, обученной на ImageNet рентгеновские снимки все будут одинаковым чёрно-белым месивом.

#### Сиамские сети, обучения по парам
 * Формируем похожие и непохожие пары. Для похожих передаём метку `1`, для непохожих `-1`. Минимизируем расстояние, умноженное на метку.
 * Проблема семплирования негативных пар.

#### Триплеты
 * Триплеты как способ лучше семплировать негативные примеры.
 * Hinge loss, отсылка к SVM.
 * Hard и semi-hard negative sampling.

#### Другие обучения метрики
 * Семплирование внутри батча.
 * Правильное формирование батча.

#### Назад к классификации
 * Помогает ли подходы обучения метрики? Да, но...
   * На больших выборках они сходятся очень долго.
   * Семплирование делать очень накладно (надо пересчитывать признаки).
 * И на самом деле не всегда лучше (ссылка на статью).

#### Angular loss
 * Можно вернуться к кроссэнтропии, и оптимизировать напрямую угол, отнормировав дескрипторы и матрицу `W`.
 * Ещё можно добавить зазор в разных формах (sphere hace, arcface, ...)

## Локальные дескрипторы.

#### Локальность признаков
 * Если не векторизовывать всё изображение, а только часть?
 * А ещё можно описать изображение через наличие особых точек.

#### SIFT-like
 * ...

#### Нейросети
 * SuperPoint
 * R2D2

### Матчинг дескрипторов, SuperGLUE.

## Ускорение поиска

## Другие способы

 * Картинка -> текст -> картинка (семантический поиск)
 * ...

## Когда что применять

### Поиск похожих картинок

### Сопоставление картинок

## Ресурсы

 * https://www.clker.com
